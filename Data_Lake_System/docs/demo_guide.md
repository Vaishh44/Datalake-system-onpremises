# üé§ Data Lakehouse Demo Guide

Use this script to present your Local Data Lakehouse to stakeholders or interviewers.

---

## üèÅ Preparation (Before you start)

1.  **Clean Slate** (Optional but recommended):
    *   Restart Docker: `docker-compose down && docker-compose up -d`
    *   Clear old data (if you want to show fresh creation):
        *   Open `localhost:9001` (MinIO Console).
        *   Login: `minioadmin` / `minioadmin`.
        *   Delete contents of `warehouse` bucket.

2.  **Start the API**:
    *   Open Terminal 1:
        ```bash
        cd api
        uvicorn main:app --reload --host 0.0.0.0 --port 8000
        ```

3.  **Open Browser or Postman**:
    *   **Option A: Swagger UI**: `http://<server-ip>:8000/docs` (e.g., [http://172.203.228.211:8000/docs](http://172.203.228.211:8000/docs))
    *   **Option B: Postman (Recommended if Browser fails)**:
        *   Download `datalake_postman_collection.json` from the repo.
        *   Import into Postman.
        *   Use the pre-configured requests.
    *   **MinIO Console**: `http://<server-ip>:9001` (e.g., [http://172.203.228.211:9001](http://172.203.228.211:9001))

---

## üé¨ Act 1: The Problem & Solution
**Say**: "Traditional Data Lakes are messy 'swamps' of files. Traditional Warehouses are expensive and rigid. I built a **Local Data Lakehouse** that combines the best of both: ACID transactions and SQL support on low-cost object storage."

---

## üé¨ Act 2: Dynamic Ingestion (The "Wow" Factor)
**Say**: "Let's see it in action. I have a raw CSV file of Products, but I haven't defined any schema in my database. Watch how the system handles it."

1.  Go to **Swagger UI** (`/docs`).
2.  Expand **POST /ingest/{table_name}**.
3.  Click **Try it out**.
4.  **table_name**: enter `products`
5.  **file**: Upload `products.csv` (You can create a dummy one or use the one generated by `test_dynamic.py`).
    *   *If you need a file, copy data from `test_dynamic.py` into a notepad and save as `products.csv`*.
6.  Click **Execute**.
7.  **Show Result**:
    *   Point to the response: `"message": "Successfully created data to default.products"`.
    *   **Say**: "The system analyzed the CSV, inferred the datatypes (Strings, Ints, Booleans), *created* the Iceberg table, and committed the data. Instantly."

8.  **Show Storage (MinIO)**:
    *   Go to MinIO Console -> `warehouse` bucket -> `default` -> `products`.
    *   Show the `data/` folder (Parquet files) and `metadata/` folder (JSON/Avro).
    *   **Say**: "Here is the data, physically stored as optimized Parquet files, managed by Apache Iceberg."

---

## üé¨ Act 3: Universal Querying (Serverless SQL)
**Say**: "Now that data is stored, we don't need complex ETL to read it. We can query it naturally."

1.  Go to **Swagger UI**.
2.  Expand **GET /query**.
3.  Click **Try it out**.
4.  **sql**: Type `SELECT * FROM products WHERE price > 50 ORDER BY price DESC`
5.  Click **Execute**.
6.  **Show Response**:
    *   Show the JSON result sorted by price.
    *   **Say**: "I'm using standard SQL. Under the hood, DuckDB is reading those Parquet files directly using the Iceberg metadata. no servers to provision."

---

## üé¨ Act 4: Advanced Features (Time Travel)
**Say**: "What if I accidentally delete data? In a normal DB, it's gone. In my Lakehouse, we have Time Travel."

1.  Open **Terminal 2**.
2.  Run your time travel demo script:
    ```bash
    python time_travel.py
    ```
    *(Make sure `time_travel.py` is configured to query the `sales` table or generic table you have data for).*
3.  **Explain Output**:
    *   "See here? I can query `Snapshot A` and see 3 rows."
    *   "Then I deleted a row in `Snapshot B`."
    *   "But I can still query `TIMESTAMP AS OF Snapshot A` and see the deleted data."

---

## üé¨ Act 5: Conclusion
**Say**: "References? Documentation? I have it all."

1.  Show the `docs/` folder in VS Code.

---

## üîß Troubleshooting: "Request Timed Out"

If you cannot connect to the API or MinIO from your laptop (Browser/Postman) but it works on the server:

### Cause 1: Firewall / Security Group
Your cloud server (AWS/Azure/GCP) likely blocks ports `8000` and `9001`.
*   **Fix**: Go to your Cloud Console -> Security Group -> **Add Inbound Rule**.
    *   **Type**: Custom TCP
    *   **Port Range**: `8000` and `9001` (and `9000` if needed).
    *   **Source**: `0.0.0.0/0` (Anywhere) or your specific IP.

### Cause 2: SSH Tunnel (The Backpack Fix)
If you cannot change firewall rules, use SSH Tunneling to forward the ports to your localhost.

Run this on your **Local Machine's Terminal**:
```bash
# syntax: ssh -L local_port:localhost:remote_port user@remote_ip
ssh -L 8000:localhost:8000 -L 9001:localhost:9001 ubuntu@172.203.228.211
```
*   Leave this terminal open.
*   Now you can use **http://localhost:8000** and **http://localhost:9001** in your Browser/Postman!

