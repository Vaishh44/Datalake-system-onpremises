# Local Data Lakehouse (MinIO + Iceberg + DuckDB + FastAPI)

A lightweight, local Data Lakehouse implementation demonstrating ACID transactions, Time Travel, and Schema Evolution on a laptop.

## ğŸ—ï¸ Architecture

| Component | Technology | Role |
|-----------|------------|------|
| **Storage** | **MinIO** | S3-compatible Object Storage (runs in Docker). |
| **Table Format** | **Apache Iceberg** | Provides ACID transactions, Schema Evolution, and Time Travel. |
| **Query Engine** | **DuckDB & Trino** | DuckDB for local analytics, Trino for distributed SQL. |
| **API Layer** | **FastAPI** | REST API for Ingestion and Querying. |

## ğŸš€ Prerequisites

1.  **Docker Desktop** (for MinIO)
2.  **Python 3.9+**
3.  **Git**

## ğŸ› ï¸ Setup Guide

### 1. Clone & Install Dependencies
```bash
git clone https://github.com/Saksheee1408/Data_Lake_System.git
cd Data_Lake_System
pip install -r requirements.txt
pip install -r api/requirements.txt
```

### 2. Start Infrastructure
Start the MinIO object storage:
```bash
docker-compose up -d
```
*   **Console**: [http://localhost:9001](http://localhost:9001) (User/Pass: `minioadmin`)
*   **API**: [http://localhost:9000](http://localhost:9000)

### 3. Initialize Bucket
Create the `warehouse` bucket:
```bash
python setup_infra.py
```

## ğŸƒ Usage: CLI Tool (`lake_cli.py`)

A unified CLI is provided to interact with the Lakehouse API.

| Command | Usage | Description |
|---------|-------|-------------|
| **Ingest** | `python lake_cli.py ingest data/leads-100.csv sales` | Uploads CSV to `sales` table. |
| **Read** | `python lake_cli.py read sales --limit 10` | Queries the table using the API. |
| **Delete** | `python lake_cli.py delete sales 1,2,3 id` | Deletes rows by ID. |

## ğŸŒ Usage: Backend API

Expose the Lakehouse via a REST API.

### 1. Start the Server
```bash
cd api
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```
*   **Swagger Docs**: [http://localhost:8000/docs](http://localhost:8000/docs)

### 2. Test the API End-to-End
Open a new terminal (keep uvicorn running) and run the automated test:
```bash
python tests/test_api.py
```
This script will:
1.  Generate a test CSV.
2.  POST it to `/upload`.
3.  GET `/query` to verify data availability.

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py              # FastAPI Application
â”‚   â””â”€â”€ requirements.txt     # API Dependencies
â”œâ”€â”€ docker-compose.yml       # MinIO & Hive Metastore & Trino Services
â”œâ”€â”€ hive_trino_setup/        # Configuration for Hive/Trino
â”œâ”€â”€ spark_jobs/              # Spark Ingestion Scripts
â”œâ”€â”€ lake_cli.py              # CLI Tool for API
â”œâ”€â”€ db_connection.py         # DuckDB Connection Helper
â”œâ”€â”€ tests/                   # Verification Scripts (test_api.py, etc.)
â”œâ”€â”€ requirements.txt         # Core Dependencies
â””â”€â”€ archive/                 # Legacy Scripts (create_table.py, etc.)
```

## ğŸ“ Design Notes
- **Catalog**: Uses **Hive Metastore** to track table state, enabling Time Travel and Schema Evolution.
- **Copy-on-Write**: Deletes and Updates rewrite data files to ensure atomicity (Iceberg V1/V2 spec).
- **Concurrency**: DuckDB is embedded; for high concurrency, consider running it in read-only mode or using a catalog service like Nessie.
